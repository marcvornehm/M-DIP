{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8b6def-f2fa-4554-a8f6-94c60b9d9eea",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06152c58-c794-4dd5-b66f-d74e80762f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random as rn\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sigpy.mri\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.python.keras.layers import Conv1D, Conv2D, Dropout, InputSpec, Layer, ReLU, UpSampling2D\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "from dip import fft_np as fft\n",
    "from dip import evaluate, mri, plotting\n",
    "from dip.dataset import MRDDataset, PhantomDataset\n",
    "\n",
    "# Make sure tensorflow is loaded properly\n",
    "print(tf.__version__)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75349a14-0dc1-424c-8e7f-a65447503d3c",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35801b98",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "if 'params' not in locals():\n",
    "    params = {}\n",
    "for param in params:\n",
    "    del locals()[param]\n",
    "_cur_locals = list(locals().keys())\n",
    "\n",
    "# these parameters are modifiable using papermill\n",
    "raw_folder = './data'\n",
    "out_folder = './results'\n",
    "filename = '<enter_filename_here>.h5'\n",
    "slice_idx = 0\n",
    "n_coils = 12\n",
    "rankk = 64                # subspace rank\n",
    "inputDepth = 32           # feature maps in the input to the DIP\n",
    "batch = 8                 # number of frames in each minibatch\n",
    "expw = 0.95               # takes exponential moving average of DIP output every epoch\n",
    "dropoutPerc = 0.05        # dropout level (number between 0-1) important for reducing noise (0.1 for 0.55T and 0.05 for 1.5T)\n",
    "gradClip = 0.01           # gradient clipping\n",
    "numEpochs = 3000          # total number of epochs\n",
    "saveFreq = 3000           # set this equal to numEpochs to save only the final result; otherwise set to a smaller number to save results every X epochs\n",
    "showFreq = 50             # display images every 50 epochs\n",
    "lr = 0.001                # learning rate\n",
    "levelsV = 5               # number of downsampling steps in temporal basis network\n",
    "cuda_num = 0              # GPU number\n",
    "phantom_acceleration = 8  # only used for mrxcat data\n",
    "phantom_snr = 10          # in dB, only used for mrxcat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameters to dictionary\n",
    "# make sure this is the first thing after the parameters cell, but in a different cell so that it works with papermill\n",
    "params = {k: v for k, v in locals().items() if k not in _cur_locals and not k.startswith('_')}\n",
    "\n",
    "# create output folder\n",
    "output_path = Path(out_folder) / f'{Path(filename).stem}' / f'slice_{slice_idx:02d}'\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(cuda_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f795ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump parameterization\n",
    "with open(output_path / 'params_lrdip.yaml', 'w') as f:\n",
    "    yaml.dump_all(\n",
    "        [{'params': params},],\n",
    "        f,\n",
    "        explicit_start=True,\n",
    "        default_flow_style=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1dd38",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8600e95-3539-4730-ad71-5c1e782f0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filename.split('.')[-1] == 'h5':\n",
    "    data = MRDDataset(Path(raw_folder) / filename, apodize=True)\n",
    "elif filename.split('.')[-1] == 'mat':\n",
    "    data = PhantomDataset(Path(raw_folder) / filename, apodize=True, acceleration_rate=phantom_acceleration, snr=phantom_snr)\n",
    "else:\n",
    "    raise ValueError('Unknown file format')\n",
    "\n",
    "# crop readout oversampling\n",
    "print('Cropping readout oversampling...')\n",
    "data.crop_readout_oversampling()\n",
    "\n",
    "# whiten k-space\n",
    "print('Whitening...')\n",
    "data.whiten()\n",
    "\n",
    "print(f'Number of slices: {data.n_slices}')\n",
    "print(f'Number of frames: {data.n_phases}')\n",
    "print(f'Number of coils:  {data.n_coils}')\n",
    "print(f'Matrix size:      {data.matrix_size}')\n",
    "\n",
    "Nx = data.matrix_size[0]\n",
    "Ny = data.matrix_size[1]\n",
    "Nex = data.n_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3812378-633a-4c9b-bd06-4c9af7902808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select slice\n",
    "data.sl = slice_idx\n",
    "\n",
    "# undersampled k-space data\n",
    "k = data.k  # [frame, coil, kx, ky]\n",
    "\n",
    "# sampling mask\n",
    "m = (np.abs(k) > 0).astype(np.int8)  # [frame, coil, kx, ky]\n",
    "m = m[:, 0]  # [frame, kx, ky]\n",
    "\n",
    "# acceleration rate\n",
    "m_tmp = m[:, m.shape[1]//2, :]\n",
    "r = m_tmp.size / m_tmp.sum()\n",
    "print(f'Acceleration rate: {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f7d35-a560-4102-8ce2-5310798d39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coil compression\n",
    "if k.shape[1] > n_coils:\n",
    "    print(f'Coil compression: {k.shape[1]} coils -> {n_coils} coils')\n",
    "    k = mri.coil_compression(k, n_coils, ch_axis=1)\n",
    "\n",
    "# plot coil-combined image\n",
    "kspace_avg = mri.average_data(k, 0)\n",
    "img_avg = fft.ifftnc(kspace_avg, axes=[1, 2])\n",
    "img_combined = mri.rss(img_avg, 0)  # type: ignore\n",
    "plt.imshow(img_combined, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# ESPIRiT\n",
    "sens_maps = sigpy.mri.app.EspiritCalib(kspace_avg, calib_width=24, thresh=0.02, crop=0, show_pbar=False).run()\n",
    "assert isinstance(sens_maps, np.ndarray), 'ESPIRiT failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5598b-e294-46d8-af14-e3d6a2ce6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = k.copy().transpose(1, 2, 3, 0)  # [coils, x, y, frames]\n",
    "coilmaps = sens_maps.copy()\n",
    "\n",
    "imunder = np.sum(fft.ifftnc(k, axes=[2, 3]) * coilmaps[None].conj(), axis=1)\n",
    "\n",
    "print(\"Normalizing data\")\n",
    "maxValue = np.max(abs(imunder))\n",
    "DATA /= maxValue # scale k-space data so that the max signal in the images is 1.\n",
    "del imunder\n",
    "gc.collect()\n",
    "\n",
    "print(\"Converting data to Tensorflow format\")\n",
    "DATA = tf.convert_to_tensor(DATA,tf.complex64) # [coils, x, y, TRs]\n",
    "DATA = tf.transpose(DATA,[0,3,1,2])  #[coils, TRs, x, y]\n",
    "coilmaps = tf.convert_to_tensor(coilmaps,tf.complex64)\n",
    "MASK = tf.convert_to_tensor(m,tf.complex64)  # [TRs, x, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc267d-ce10-4366-911b-541ad4ee1aea",
   "metadata": {},
   "source": [
    "# LR-DIP Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f2b63-a0c3-4dc3-9579-1267fd2ebd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "learnRates = np.ones((numEpochs+1,)) * lr\n",
    "learnRates[0:10] = np.linspace(1e-4,learnRates[-1],10) # use a smaller learning rate for the first few epochs\n",
    "\n",
    "# The temporal u-net has 5 encoding/decoding layers.\n",
    "# Each encoding layer downsamples by a factor of 2.\n",
    "# Here we make sure that the number of frames is divisible by 2^5 = 32.\n",
    "# If not, just pad it with zeros before inputting to the u-net, and then remove\n",
    "# the padded frames afterwards.\n",
    "upper_frames = int( np.ceil(Nex/32) * 32 )\n",
    "difference_frames = upper_frames - Nex\n",
    "left_pad = difference_frames//2\n",
    "right_pad = difference_frames - left_pad\n",
    "\n",
    "# and same for the spatial u-net\n",
    "upper_x = int( np.ceil(Nx/32) * 32 )\n",
    "upper_y = int( np.ceil(Ny/32) * 32 )\n",
    "difference_x = upper_x - Nx\n",
    "difference_y = upper_y - Ny\n",
    "left_pad_x = difference_x//2\n",
    "right_pad_x = difference_x - left_pad_x\n",
    "left_pad_y = difference_y//2\n",
    "right_pad_y = difference_y - left_pad_y\n",
    "\n",
    "nsteps = Nex // batch    # number of iterations per epoch (1 epoch = 1 pass over all time frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b21b99",
   "metadata": {},
   "source": [
    "## Initialize the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68bdbb-98d4-4342-ad8d-6d697193b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionPadding1D(Layer):\n",
    "    def __init__(self, padding=(1,), data_format='channels_first', **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        self.data_format = data_format\n",
    "        super(ReflectionPadding1D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_first\" configuration\"\"\"\n",
    "        if self.data_format == 'channels_first':\n",
    "            return (s[0], s[1], s[1] + 2 * self.padding[0])\n",
    "        elif self.data_format == 'channels_last':\n",
    "            return (s[0], s[1] + 2 * self.padding[0], s[2])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        h_pad = self.padding[0]\n",
    "        if self.data_format == 'channels_first':\n",
    "            return tf.pad(x, [[0,0], [0,0], [h_pad,h_pad] ], 'REFLECT')\n",
    "        elif self.data_format == 'channels_last':\n",
    "            return tf.pad(x, [[0,0], [h_pad,h_pad], [0,0] ], 'REFLECT')\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), data_format='channels_first', **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        self.data_format = data_format\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_first\" configuration\"\"\"\n",
    "        if self.data_format == 'channels_first':\n",
    "            return (s[0], s[1], s[2] + 2 * self.padding[0], s[3] + 2 * self.padding[1])\n",
    "        elif self.data_format == 'channels_last':\n",
    "            return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        if self.data_format == 'channels_first':\n",
    "            return tf.pad(x, [[0,0], [0,0], [h_pad,h_pad], [w_pad,w_pad] ], 'REFLECT')\n",
    "        elif self.data_format == 'channels_last':\n",
    "            return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad],[0,0] ], 'REFLECT')\n",
    "\n",
    "class MyConv1D(Model):\n",
    "    def __init__(self, myFilterSize=128, myKernelSize=(3,), myKernelStride=(1,), \\\n",
    "                 myDropoutType='Dropout', myDropoutFraction=0.05):\n",
    "        super(MyConv1D, self).__init__()\n",
    "\n",
    "        self.conv = Conv1D(filters=myFilterSize, kernel_size=myKernelSize, strides=myKernelStride, \\\n",
    "                           padding='valid', data_format='channels_first')\n",
    "        self.reflect = ReflectionPadding1D(padding=(myKernelSize[0]//2,))\n",
    "        self.drop = Dropout(myDropoutFraction) if myDropoutType == 'Dropout' else SpatialDropout2D(myDropoutFraction)\n",
    "        self.act = ReLU()\n",
    "\n",
    "    def call(self, x, trainingDrop=True):\n",
    "            x = self.reflect(x)\n",
    "            x = self.drop(x, training=trainingDrop)\n",
    "            x = self.conv(x)\n",
    "            x = self.act(x)\n",
    "            return x\n",
    "\n",
    "class MyConv2D(Model):\n",
    "    def __init__(self, myFilterSize=128, myKernelSize=(3, 3), myKernelStride=(1, 1), \\\n",
    "                 myDropoutType='Dropout', myDropoutFraction=0.05):\n",
    "        super(MyConv2D, self).__init__()\n",
    "\n",
    "        self.conv = Conv2D(filters=myFilterSize, kernel_size=myKernelSize, strides=myKernelStride, \\\n",
    "                           padding='valid', data_format='channels_first')\n",
    "        self.reflect = ReflectionPadding2D(padding=(myKernelSize[0]//2,myKernelSize[1]//2))\n",
    "        self.drop = Dropout(myDropoutFraction) if myDropoutType == 'Dropout' else SpatialDropout2D(myDropoutFraction)\n",
    "        self.act = ReLU()\n",
    "\n",
    "    def call(self, x, trainingDrop=True):\n",
    "            x = self.reflect(x)\n",
    "            x = self.drop(x, training=trainingDrop)\n",
    "            x = self.conv(x)\n",
    "            x = self.act(x)\n",
    "            return x\n",
    "\n",
    "class SpatialBasisNetwork(Model):\n",
    "    def __init__(self, ups=5*[2], filters=5*[128], skips=5*[4]):\n",
    "        super(SpatialBasisNetwork, self).__init__()\n",
    "\n",
    "        # The output has 2 channels for the real and imaginary parts of the K subspace images.\n",
    "        self.convOut = Conv2D(2*rankk, (1, 1), (1, 1),data_format='channels_first')\n",
    "\n",
    "        self.ups = ups\n",
    "        self.filters = filters\n",
    "        self.skips = skips\n",
    "\n",
    "        self.nlayers   = len(ups)\n",
    "        self.encoder1  = {}\n",
    "        self.encoder2  = {}\n",
    "        self.decoder1  = {}\n",
    "        self.decoder2  = {}\n",
    "        self.skips     = {}\n",
    "        self.upsample  = {}\n",
    "\n",
    "        for i in range(self.nlayers):\n",
    "            self.encoder1[i] = MyConv2D(filters[i],(3,3),(ups[i],ups[i]),'Dropout',dropoutPerc)\n",
    "            self.encoder2[i] = MyConv2D(filters[i],(3,3),(1,1),'Dropout',dropoutPerc)\n",
    "            self.skips[i]    = MyConv2D(skips[i],(1,1),(1,1),'Dropout',dropoutPerc)\n",
    "            self.decoder1[i] = MyConv2D(filters[i],(3,3),(1,1),'Dropout',dropoutPerc)\n",
    "            self.decoder2[i] = MyConv2D(filters[i],(3,3),(1,1),'Dropout',dropoutPerc)\n",
    "            self.upsample[i]  = UpSampling2D( size=(ups[i],ups[i]),interpolation='nearest',data_format='channels_first')\n",
    "\n",
    "    def call(self, x, trainingDrop=True):\n",
    "\n",
    "        # Encoder\n",
    "        xskips = {}\n",
    "        for i in range(self.nlayers):\n",
    "            xskips[i] = self.skips[i](x, trainingDrop=trainingDrop)\n",
    "            x = self.encoder1[i](x, trainingDrop=trainingDrop)\n",
    "            x = self.encoder2[i](x, trainingDrop=trainingDrop)\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(self.nlayers-1,-1,-1):\n",
    "            x = self.upsample[i](x)\n",
    "            x = tf.concat([x,xskips[i]], axis=1)\n",
    "            x = self.decoder1[i](x, trainingDrop=trainingDrop)\n",
    "            x = self.decoder2[i](x, trainingDrop=trainingDrop)\n",
    "\n",
    "        x = self.convOut(x) # [batch, 2*K, X, Y]\n",
    "        x = tf.transpose(x,[0,2,3,1]) # [batch, X, Y, 2*K]\n",
    "        return x\n",
    "\n",
    "class TemporalBasisNetwork(Model):\n",
    "    def __init__(self, ups=5*[2], filters=5*[128], skips=5*[4]):\n",
    "        super(TemporalBasisNetwork, self).__init__()\n",
    "\n",
    "        # The output has 2 channels for the real and imaginary parts of the K subspace images.\n",
    "        self.convOut = Conv1D(2*rankk, (1,), (1,),data_format='channels_first')\n",
    "\n",
    "        self.ups = ups\n",
    "        self.filters = filters\n",
    "        self.skips = skips\n",
    "\n",
    "        self.nlayers   = len(ups)\n",
    "        self.encoder1  = {}\n",
    "        self.encoder2  = {}\n",
    "        self.decoder1  = {}\n",
    "        self.decoder2  = {}\n",
    "        self.skips     = {}\n",
    "        self.upsample  = {}\n",
    "\n",
    "        for i in range(self.nlayers):\n",
    "            self.encoder1[i] = MyConv1D(filters[i],(3,),(ups[i],),'Dropout',dropoutPerc)\n",
    "            self.encoder2[i] = MyConv1D(filters[i],(3,),(1,),'Dropout',dropoutPerc)\n",
    "            self.skips[i]    = MyConv1D(skips[i],(1,),(1,),'Dropout',dropoutPerc)\n",
    "            self.decoder1[i] = MyConv1D(filters[i],(3,),(1,),'Dropout',dropoutPerc)\n",
    "            self.decoder2[i] = MyConv1D(filters[i],(3,),(1,),'Dropout',dropoutPerc)\n",
    "            self.upsample[i]  = UpSampling2D( size=(1,ups[i]),interpolation='nearest',data_format='channels_first')\n",
    "\n",
    "    def call(self, x, trainingDrop=True):\n",
    "\n",
    "        # Encoder\n",
    "        xskips = {}\n",
    "        for i in range(self.nlayers):\n",
    "            xskips[i] = self.skips[i](x, trainingDrop=trainingDrop)\n",
    "            x = self.encoder1[i](x, trainingDrop=trainingDrop)\n",
    "            x = self.encoder2[i](x, trainingDrop=trainingDrop)\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(self.nlayers-1,-1,-1):\n",
    "            x = self.upsample[i](x[:,:,None,:])[:,:,0,:]\n",
    "            x = tf.concat([x,xskips[i]], axis=1)\n",
    "            x = self.decoder1[i](x, trainingDrop=trainingDrop)\n",
    "            x = self.decoder2[i](x, trainingDrop=trainingDrop)\n",
    "\n",
    "        x = self.convOut(x)\n",
    "        x = tf.transpose(x,[0,2,1])\n",
    "        return x\n",
    "\n",
    "modelU = SpatialBasisNetwork() # u-net that generates spatial basis images\n",
    "modelV = TemporalBasisNetwork(ups=levelsV*[2], filters=levelsV*[128], skips=levelsV*[4])\n",
    "\n",
    "# initialize the network inputs.\n",
    "# these are initialized with random values and remain fixed during training\n",
    "input_spatial = tf.random.uniform([1,inputDepth,upper_x,upper_y],minval = -1,maxval = 1)\n",
    "input_temporal = tf.random.uniform([1,inputDepth,upper_frames],minval = -1,maxval = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a7e5d-d45f-491c-b543-68afabd0042f",
   "metadata": {},
   "source": [
    "## Network Inference\n",
    "Here we define a function that keeps track of the DIP output after every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb03ea6-f248-47d8-b569-092e17155213",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uavg = tf.zeros((Nx,Ny,rankk),dtype=tf.complex64) # the final spatial basis functions\n",
    "Vavg = tf.zeros((rankk,Nex),dtype=tf.complex64) # the final temporal basis function\n",
    "\n",
    "@tf.function\n",
    "def TestNetwork(Uavg,Vavg):\n",
    "    # spatial basis functions\n",
    "    U = modelU(input_spatial,trainingDrop=False)\n",
    "    U = tf.cast(tf.complex(U[0,left_pad_x:left_pad_x+Nx,left_pad_y:left_pad_y+Ny,0:rankk],\\\n",
    "                           U[0,left_pad_x:left_pad_x+Nx,left_pad_y:left_pad_y+Ny,rankk:]),tf.complex64)  # [X, Y, rank]\n",
    "\n",
    "    # temporal basis functions\n",
    "    V = modelV(input_temporal,trainingDrop=False)\n",
    "    V = tf.cast(tf.complex(V[0,left_pad:left_pad+Nex,0:rankk],V[0,left_pad:left_pad+Nex,rankk:]),tf.complex64)\n",
    "    V = tf.transpose(V)\n",
    "\n",
    "    # take the exponential weighted average of the previous runs\n",
    "    return U*(1-expw)+Uavg*expw, V*(1-expw)+Vavg*expw\n",
    "\n",
    "# Let's time how long it takes to apply the network. The first run takes longer, so time it on the 2nd run\n",
    "Uavg,Vavg = TestNetwork(Uavg,Vavg)\n",
    "startTime = time.time()\n",
    "Uavg,Vavg = TestNetwork(Uavg,Vavg)\n",
    "inferenceTime = time.time() - startTime\n",
    "print(\"Inference time = {:.1f} ms\".format(inferenceTime*1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117dd2e2-d8b1-470e-b261-9ef07ccc58cc",
   "metadata": {},
   "source": [
    "## Define the network training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd26dc3-a9b5-4db7-b85b-15cb615610e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam( learnRates[0] )  # optimizer for the image reconstruction network\n",
    "\n",
    "@tf.function\n",
    "def trainDeepImagePrior(TRIndex_Batch, loss_scale=1.):\n",
    "\n",
    "    # acquired (undersampled) spiral k-space data (gather the time frames for the current minibatch)\n",
    "    acquiredData = tf.gather(DATA, TRIndex_Batch, axis=1)  # [coils, timepoints, x, y]\n",
    "    batchMask = tf.gather(MASK, TRIndex_Batch, axis=0)[None]  # [1, timepoints, x, y]\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        # compute the spatial basis functions\n",
    "        U = modelU(input_spatial,trainingDrop=True)[0,:,:,:] # [x-pixels, y-pixels, 2*K]. Note: K is the subspace rank\n",
    "        U = tf.cast(tf.complex(U[left_pad_x:left_pad_x+Nx,left_pad_y:left_pad_y+Ny,0:rankk],\n",
    "                               U[left_pad_x:left_pad_x+Nx,left_pad_y:left_pad_y+Ny,rankk:]),tf.complex64) # [x-pixels, y-pixels, K]\n",
    "\n",
    "        # compute the temporal basis functions\n",
    "        V = modelV(input_temporal,trainingDrop=True)[0,:,:] # [frames, 2*K]\n",
    "        V = tf.cast(tf.complex(V[left_pad:left_pad+Nex,0:rankk],V[left_pad:left_pad+Nex,rankk:]),tf.complex64)\n",
    "        V = tf.transpose(V) # [k, frames]\n",
    "        V = tf.gather(V,TRIndex_Batch,axis=1) # k, batch\n",
    "\n",
    "        # multiply U*V to get the dynamic images\n",
    "        y = tf.transpose(tf.matmul(U,V),[2,0,1]) # TRs, X, Y\n",
    "\n",
    "        # multiply by coil sensitivities\n",
    "        y = y[None,:,:,:] * coilmaps[:,None,:,:] # image size: [coils, TRs, x-pixels, y-pixels]\n",
    "\n",
    "        # forward FFT (image to k-space)\n",
    "        y = tf.signal.ifftshift(y,axes=[2,3])\n",
    "        y = tf.signal.fft2d(y) / tf.sqrt(tf.cast(tf.size(y[0, 0]), tf.complex64))\n",
    "        y = tf.signal.fftshift(y,axes=[2,3])\n",
    "\n",
    "        # compute MSE loss in kspace\n",
    "        y = batchMask * y\n",
    "        loss = tf.reduce_sum(abs(acquiredData-y)**2) * loss_scale\n",
    "\n",
    "    gradients = tape.gradient(loss, modelU.trainable_variables + modelV.trainable_variables)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 0.01)\n",
    "    optimizer.apply_gradients(zip(gradients, modelU.trainable_variables + modelV.trainable_variables))\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "# It helps to scale the loss value to a reasonable range...\n",
    "timerange = np.arange(0,Nex,1).astype('int') # list of time frames\n",
    "lstep=0\n",
    "t = timerange[lstep*batch:(lstep+1)*batch]\n",
    "timeIndex = tf.cast(tf.convert_to_tensor(t),tf.int32)\n",
    "currLoss = trainDeepImagePrior(timeIndex)\n",
    "loss_scale = 100. / currLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532b566-1056-458c-bec5-282e0ca00891",
   "metadata": {},
   "source": [
    "# DIP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57287f74-7122-465f-b4de-6e77dac146b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "losses = []        # keep running list of the loss function value\n",
    "\n",
    "for e in range(numEpochs + 1):\n",
    "\n",
    "    rn.shuffle(timerange) # shuffle the time frames\n",
    "    runningLoss = 0\n",
    "\n",
    "    optimizer.lr = learnRates[e]\n",
    "\n",
    "    for lstep in range(nsteps): # Step through all image frames for training\n",
    "\n",
    "        # Select time frames for the current minibatch\n",
    "        t = timerange[lstep*batch:(lstep+1)*batch]\n",
    "        timeIndex = tf.cast(tf.convert_to_tensor(t),tf.int32)\n",
    "\n",
    "        # Compute the loss and update the DIP weights\n",
    "        currLoss = trainDeepImagePrior(timeIndex, loss_scale)\n",
    "        runningLoss += currLoss/nsteps # update the running loss\n",
    "\n",
    "    losses.append(runningLoss)\n",
    "\n",
    "    # update our final reconstruction\n",
    "    Uavg,Vavg = TestNetwork(Uavg,Vavg)\n",
    "    imNet = tf.matmul(Uavg,Vavg)  # Multiply U*V\n",
    "    imNet *= maxValue  # scale the image back to the original scale\n",
    "\n",
    "    # Display progress so far....\n",
    "    elapsedTime = (time.time()-startTime) / 60.0 # elapsed time, in minutes\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"Epoch {} / {}: Loss {:.6g}, {:.1f} minutes\".format(e,numEpochs,losses[-1], elapsedTime))\n",
    "\n",
    "    # Display images so far....\n",
    "    if e % showFreq == 0:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        framesToShow = np.linspace(0,Nex-1,5).astype('int') # display images at a few time points while the recon is running\n",
    "\n",
    "        fs = 5\n",
    "        plt.figure(figsize=(fs*5,fs*1))\n",
    "        for lt in range(len(framesToShow)):\n",
    "            t = framesToShow[lt]\n",
    "            plt.subplot(1,len(framesToShow),lt+1)\n",
    "            plt.imshow(abs(imNet[:,:,t]),cmap='gray',vmin=0,vmax=0.85*np.max(abs(imNet[:,:,t])))\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Save results periodically...\n",
    "    is_last_epoch = (e == numEpochs)\n",
    "    if (e % saveFreq == 0 and e>0) or is_last_epoch:\n",
    "        print(\"Saving...\")\n",
    "        suffix = '' if is_last_epoch else '_epoch_{:04d}'.format(e)\n",
    "        imNet_npy = imNet.numpy().transpose(2, 0, 1)\n",
    "        if is_last_epoch:\n",
    "            imNet_npy = mri.center_crop(imNet_npy, data.recon_size, (1, 2))\n",
    "            np.save(output_path / f'cine_lrdip.npy', imNet_npy)\n",
    "        plotting.save_gif(np.abs(imNet_npy), output_path / f'cine_lrdip{suffix}.gif', normalize=True,\n",
    "                          equalize_histogram=True, duration=min(round(data.tres), 200))\n",
    "\n",
    "        Uavg_np = Uavg.numpy()\n",
    "        plotting.plot_multichannel(Uavg_np, channel_axis=2, columns=5, complex='abs', figheight_per_row=4,\n",
    "                                   save_path=output_path / f'lrdip_U{suffix}', show=is_last_epoch, cmap='gray')\n",
    "\n",
    "        Vavg_np = Vavg.numpy()\n",
    "        plotting.plot_stacked(Vavg_np, channel_axis=0, save_path=output_path / f'lrdip_V{suffix}', show=is_last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_tmp = [float(x) for x in losses]\n",
    "plt.semilogy(losses_tmp, linewidth=0.6)\n",
    "plt.ylim(bottom=5e-1, top=1e2)\n",
    "plt.title('Loss')\n",
    "plt.savefig(output_path / 'loss_lrdip.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9e58f",
   "metadata": {},
   "source": [
    "# Only for phantom data: Quantitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(data, PhantomDataset):\n",
    "    # get magnitude reconstructions\n",
    "    cine_gt = np.abs(data.ground_truth[slice_idx])\n",
    "    cine_lrdip = np.abs(imNet_npy)\n",
    "\n",
    "    # quantitative evaluation\n",
    "    with open('mrxcat_annotations.yaml', 'r') as f:\n",
    "        annotations = yaml.safe_load(f)[output_path.parent.name]\n",
    "        bbox = annotations['bbox']\n",
    "        center = annotations['center']\n",
    "    metrics_cine = evaluate.get_metrics(cine_gt, ('LR-DIP', cine_lrdip))\n",
    "    metrics_roi = evaluate.get_metrics(cine_gt, ('LR-DIP', cine_lrdip), bbox=bbox)\n",
    "    metrics_profiles = evaluate.get_metrics(cine_gt, ('LR-DIP', cine_lrdip), center=center)\n",
    "    with pd.option_context('display.float_format', '{:.4f}'.format):\n",
    "        print('Cine:')\n",
    "        print(metrics_cine)\n",
    "        print('\\nROI:')\n",
    "        print(metrics_roi)\n",
    "        print('\\nTemporal Profiles:')\n",
    "        print(metrics_profiles)\n",
    "\n",
    "    # save metrics to csv\n",
    "    evaluate.update_metrics_csv(\n",
    "        output_path / 'metrics.csv', ('cine', metrics_cine), ('roi', metrics_roi), ('profiles', metrics_profiles),\n",
    "    )\n",
    "\n",
    "    # save ROI\n",
    "    evaluate.save_cine_roi(cine_lrdip, bbox, output_path / 'ROI_cine_lrdip.gif', min(data.tres, 200))\n",
    "\n",
    "    # save temporal profiles\n",
    "    evaluate.save_temporal_profiles(cine_lrdip, center, output_path / 'profile_cine_lrdip.png')\n",
    "\n",
    "    # save error image\n",
    "    evaluate.save_error_map(cine_gt, cine_lrdip, output_path / 'error10x_lrdip.gif', min(data.tres, 200), scale=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9323bf-bcf2-4270-82c1-7b76cc958ac0",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
